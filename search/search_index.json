{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p> is a Python library containing reference implementations of a bunch of very useful unsupervised learning algorithms that you probably won't find elsewhere.</p>"},{"location":"#what-is","title":"What  is:","text":"<ul> <li>A collection of unsupervised machine learning algorithms</li> <li>A scikit-learn compatible library</li> <li>An educational resource containing worked examples and reference implementation</li> </ul>"},{"location":"#what-isnt","title":"What  isn't:","text":"<ul> <li>The most feature-complete or efficient implementation of these algorithms</li> <li>A replacement for scikit-learn</li> <li>An all-in-one machine learning framework</li> <li>A library for complete Bayesian inference. Use a PPL like NumPyro, PyMC or Stan.</li> </ul>"},{"location":"#basic-usage","title":"Basic usage","text":"<p>Install noloox from PyPI:</p> <pre><code>pip install noloox\n</code></pre> <p>Then you can load models from the library and use them the same way you would use scikit-learn.</p> <pre><code>from noloox.mixture import StudentsTMixture\n\nmodel = StudentsTMixture(n_components=10)\ncluster_labels = model.fit_predict(X)\n</code></pre>"},{"location":"#models","title":"Models","text":"Model What do I use it for? JAX or NumPy? What algorithm? Tutorial Peax Cluster 2D data where the number of clusters is unknown. NumPy Expectation-Maximization Finding the number of clusters in the data SNMF Factor data, where you expect the factors to be non-negative, but the data is unbounded JAX Iterative updates Topic discovery by factoring transformer embeddings WNMF NMF, but you don't want to weight all observations equally. NumPy Iterative updates - StudentsTMixture/CauchyMixture Cluster continuous data in a way that is robust to outliers. JAX Expectation-Maximization Outlier-Robust Clustering DirichletMultinomialMixture Cluster count data/Short-text topic modelling JAX Collapsed Gibbs Sampling Topic modelling for short texts and Clustering Count Data"},{"location":"#our-philosophy-and-goals","title":"Our philosophy and goals","text":"<ul> <li>Keep implementations simple and minimal, Minimal dependencies</li> <li>Everything should either be implemented in NumPy or JAX. Preferably as many in JAX as possible.</li> <li>Library structure should match sklearn standards, and all algorithms should be drop-in replacements for scikit-learn equivalents.</li> <li>Under these restrictions, algorithms should be as fast as humanly possible</li> </ul>"},{"location":"#the-wishlist","title":"The  wishlist:","text":"<p>There are a number of algorithms that would be nice to implement in the library. Contributions are very welcome.</p> <ul> <li>ProdLDA, and amortized ProdLDA (CTMs) (without Flax)</li> <li>Parametric-TSNE, possibly also Multi-scale Parametric-TSNE</li> <li>DiRE</li> <li>Infinite NMF</li> <li>Latent Dirichlet Allocation with Gibbs Sampling</li> <li>Gaussian LDA</li> </ul>"},{"location":"CauchyMixture/","title":"CauchyMixture","text":""},{"location":"CauchyMixture/#noloox.mixture.CauchyMixture","title":"<code>noloox.mixture.CauchyMixture</code>","text":"<p>             Bases: <code>StudentsTMixture</code></p> <p>Cauchy Mixture Model. This class allows you to estimate a multivariate mixture of Cauchys over your data. Equivalent to StudentsTMixture, except the degrees of freedom is fixed to 1.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>The number of mixture components.</p> required <code>tol</code> <code>float</code> <p>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</p> <code>1e-05</code> <code>reg_covar</code> <code>float</code> <p>Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.</p> <code>1e-06</code> <code>max_iter</code> <code>int</code> <p>The number of EM iterations to perform.</p> <code>1000</code> <p>Attributes:</p> Name Type Description <code>weights_</code> <code>array-like of shape (n_components,)</code> <p>The weights of each mixture components.</p> <code>means_</code> <code>array-like of shape (n_components, n_features)</code> <p>The mean of each mixture component.</p> <code>n_iter_</code> <code>int</code> <p>Number of step used by the best fit of EM to reach the convergence.</p> <code>converged_</code> <code>bool</code> <p>True when convergence of the best fit of EM was reached, False otherwise.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>class CauchyMixture(StudentsTMixture):\n    \"\"\"Cauchy Mixture Model.\n    This class allows you to estimate a multivariate mixture of Cauchys over your data.\n    Equivalent to StudentsTMixture, except the degrees of freedom is fixed to 1.\n\n    Parameters\n    ----------\n    n_components: int\n        The number of mixture components.\n    tol: float, default=1e-5\n        The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.\n    reg_covar: float, default=1e-6\n        Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.\n    max_iter: int, default=1000\n        The number of EM iterations to perform.\n\n    Attributes\n    ----------\n    weights_: array-like of shape (n_components,)\n        The weights of each mixture components.\n    means_: array-like of shape (n_components, n_features)\n        The mean of each mixture component.\n    n_iter_: int\n        Number of step used by the best fit of EM to reach the convergence.\n    converged_: bool\n        True when convergence of the best fit of EM was reached, False otherwise.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        tol: float = 1e-5,\n        reg_covar: float = 1e-06,\n        max_iter: int = 1000,\n        random_state=None,\n    ):\n        super().__init__(\n            n_components=n_components,\n            tol=tol,\n            reg_covar=reg_covar,\n            max_iter=max_iter,\n            random_state=random_state,\n            df=1.0,\n        )\n</code></pre>"},{"location":"DirichletMultinomialMixture/","title":"DirichletMultinomialMixture","text":""},{"location":"DirichletMultinomialMixture/#noloox.mixture.DirichletMultinomialMixture","title":"<code>noloox.mixture.DirichletMultinomialMixture</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClusterMixin</code>, <code>DensityMixin</code></p> <p>Implementation of the Dirichlet Multinomial Mixture Model with Gibbs Sampling solver</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of mixture components in the model.</p> required <code>n_iter</code> <code>int</code> <p>Number of iterations during fitting. If you find your results are unsatisfactory, increase this number.</p> <code>50</code> <code>alpha</code> <code>float</code> <p>Willingness of a document joining an empty cluster.</p> <code>0.1</code> <code>beta</code> <code>float</code> <p>Willingness to join clusters, where the terms in the document are not present.</p> <code>0.1</code> <code>random_state</code> <code>Optional[int]</code> <p>Random seed to use for reproducibility.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>components_</code> <code>array of shape (n_components, n_vocab)</code> <p>Describes all components of the topic distribution. Contains the amount each word has been assigned to each component during fitting.</p> <code>n_features_in_</code> <code>int</code> <p>Number of total vocabulary items seen during fitting.</p> Source code in <code>noloox/mixture/dmm.py</code> <pre><code>class DirichletMultinomialMixture(BaseEstimator, ClusterMixin, DensityMixin):\n    \"\"\"Implementation of the Dirichlet Multinomial Mixture Model with Gibbs Sampling\n    solver\n\n    Parameters\n    ----------\n    n_components: int\n        Number of mixture components in the model.\n    n_iter: int, default 50\n        Number of iterations during fitting.\n        If you find your results are unsatisfactory, increase this number.\n    alpha: float, default 0.1\n        Willingness of a document joining an empty cluster.\n    beta: float, default 0.1\n        Willingness to join clusters, where the terms in the document\n        are not present.\n    random_state: int, default None\n        Random seed to use for reproducibility.\n\n    Attributes\n    ----------\n    components_: array of shape (n_components, n_vocab)\n        Describes all components of the topic distribution.\n        Contains the amount each word has been assigned to each component\n        during fitting.\n    n_features_in_: int\n        Number of total vocabulary items seen during fitting.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        n_iter: int = 50,\n        alpha: float = 0.1,\n        beta: float = 0.1,\n        random_state: Optional[int] = None,\n    ):\n        super().__init__()\n        self.n_components = n_components\n        self.n_iter = n_iter\n        self.alpha = alpha\n        self.beta = beta\n        self.random_state = random_state\n\n    def get_params(self, deep: bool = False) -&gt; dict:\n        \"\"\"Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep: bool, default False\n            Ignored, exists for sklearn compatibility.\n\n        Returns\n        -------\n        dict\n            Parameter names mapped to their values.\n\n        Note\n        ----\n        Exists for sklearn compatibility.\n        \"\"\"\n        return {\n            \"n_components\": self.n_components,\n            \"n_iter\": self.n_iter,\n            \"alpha\": self.alpha,\n            \"beta\": self.beta,\n        }\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Fits the model using Gibbs Sampling. Detailed description of the\n        algorithm in Yin and Wang (2014).\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            BOW matrix of corpus.\n        y: None\n            Ignored, exists for sklearn compatibility.\n\n        Returns\n        -------\n        DirichletMultinomialMixture\n            The fitted model.\n\n        \"\"\"\n        if issparse(X):\n            warnings.warn(\n                \"Sparse arrays are not yet supported. Implicitly converting to dense array.\"\n            )\n            X = np.asarray(X.todense())\n        if self.random_state is not None:\n            random_key = jax.random.key(self.random_state)\n        else:\n            random_key = jax.random.key(random.randint(0, 1000))\n        random_key, self.components_, self.labels_, self.m_z, self.n_z = fit_model(\n            random_key,\n            self.n_components,\n            self.n_iter,\n            self.alpha,\n            self.beta,\n            X,\n        )\n        self.weights_ = np.asarray(self.m_z) / np.sum(self.m_z)\n        self.components_ = np.asarray(self.components_)\n        D, V = X.shape\n        self._predict_proba = jax.vmap(\n            lambda x: softmax(\n                log_cond_prob(\n                    self.m_z,\n                    self.components_,\n                    self.n_z,\n                    x,\n                    D,\n                    self.n_components,\n                    V,\n                    self.alpha,\n                    self.beta,\n                )\n            ),\n        )\n\n        return self.labels_\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        \"\"\"Predicts probabilities for each document belonging to each\n        component.\n\n        Parameters\n        ----------\n        X: array-like  of shape (n_samples, n_features)\n            Document-term matrix.\n\n        Returns\n        -------\n        array of shape (n_samples, n_components)\n            Probabilities for each document belonging to each cluster.\n\n        Raises\n        ------\n        NotFittedException\n            If the model is not fitted, an exception will be raised\n        \"\"\"\n        if not hasattr(self, \"_predict_proba\"):\n            raise NotFittedError(\"Model not fitted yet, can't predict probabilities.\")\n        if issparse(X):\n            warnings.warn(\n                \"Sparse arrays are not yet supported. Implicitly converting to dense array.\"\n            )\n            X = np.asarray(X.todense())\n        p = self._predict_proba(X)\n        return np.asarray(p)\n\n    def fit(self, X, y=None):\n        self.fit_predict(X, y)\n        return self\n\n    def transform(self, X) -&gt; np.ndarray:\n        \"\"\"Alias for predict_proba().\"\"\"\n        return self.predict_proba(X)\n\n    def predict(self, X) -&gt; np.ndarray:\n        \"\"\"Predicts cluster labels for a set of documents. Mainly exists for\n        compatibility with density estimators in sklearn.\n\n        Parameters\n        ----------\n        X: array-like  of shape (n_samples, n_features)\n            Document-term matrix.\n\n        Returns\n        -------\n        array of shape (n_samples,)\n            Cluster label for each document.\n\n        Raises\n        ------\n        NotFittedException\n            If the model is not fitted, an exception will be raised\n        \"\"\"\n        return np.argmax(self.predict_proba(X), axis=1)\n\n    def fit_transform(\n        self,\n        X,\n        y=None,\n    ) -&gt; np.ndarray:\n        \"\"\"Fits the model, then transforms the given data.\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            Document-term matrix.\n        y: None\n            Ignored, sklearn compatibility.\n\n        Returns\n        -------\n        array of shape (n_samples, n_components)\n            Probabilities for each document belonging to each cluster.\n        \"\"\"\n        return self.fit(X).transform(X)\n</code></pre>"},{"location":"DirichletMultinomialMixture/#noloox.mixture.DirichletMultinomialMixture.fit_predict","title":"<code>fit_predict(X, y=None)</code>","text":"<p>Fits the model using Gibbs Sampling. Detailed description of the algorithm in Yin and Wang (2014).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>BOW matrix of corpus.</p> required <code>y</code> <p>Ignored, exists for sklearn compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>DirichletMultinomialMixture</code> <p>The fitted model.</p> Source code in <code>noloox/mixture/dmm.py</code> <pre><code>def fit_predict(self, X, y=None):\n    \"\"\"Fits the model using Gibbs Sampling. Detailed description of the\n    algorithm in Yin and Wang (2014).\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        BOW matrix of corpus.\n    y: None\n        Ignored, exists for sklearn compatibility.\n\n    Returns\n    -------\n    DirichletMultinomialMixture\n        The fitted model.\n\n    \"\"\"\n    if issparse(X):\n        warnings.warn(\n            \"Sparse arrays are not yet supported. Implicitly converting to dense array.\"\n        )\n        X = np.asarray(X.todense())\n    if self.random_state is not None:\n        random_key = jax.random.key(self.random_state)\n    else:\n        random_key = jax.random.key(random.randint(0, 1000))\n    random_key, self.components_, self.labels_, self.m_z, self.n_z = fit_model(\n        random_key,\n        self.n_components,\n        self.n_iter,\n        self.alpha,\n        self.beta,\n        X,\n    )\n    self.weights_ = np.asarray(self.m_z) / np.sum(self.m_z)\n    self.components_ = np.asarray(self.components_)\n    D, V = X.shape\n    self._predict_proba = jax.vmap(\n        lambda x: softmax(\n            log_cond_prob(\n                self.m_z,\n                self.components_,\n                self.n_z,\n                x,\n                D,\n                self.n_components,\n                V,\n                self.alpha,\n                self.beta,\n            )\n        ),\n    )\n\n    return self.labels_\n</code></pre>"},{"location":"DirichletMultinomialMixture/#noloox.mixture.DirichletMultinomialMixture.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fits the model, then transforms the given data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Document-term matrix.</p> required <code>y</code> <p>Ignored, sklearn compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>array of shape (n_samples, n_components)</code> <p>Probabilities for each document belonging to each cluster.</p> Source code in <code>noloox/mixture/dmm.py</code> <pre><code>def fit_transform(\n    self,\n    X,\n    y=None,\n) -&gt; np.ndarray:\n    \"\"\"Fits the model, then transforms the given data.\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        Document-term matrix.\n    y: None\n        Ignored, sklearn compatibility.\n\n    Returns\n    -------\n    array of shape (n_samples, n_components)\n        Probabilities for each document belonging to each cluster.\n    \"\"\"\n    return self.fit(X).transform(X)\n</code></pre>"},{"location":"DirichletMultinomialMixture/#noloox.mixture.DirichletMultinomialMixture.get_params","title":"<code>get_params(deep=False)</code>","text":"<p>Get parameters for this estimator.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>Ignored, exists for sklearn compatibility.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Parameter names mapped to their values.</p> Note <p>Exists for sklearn compatibility.</p> Source code in <code>noloox/mixture/dmm.py</code> <pre><code>def get_params(self, deep: bool = False) -&gt; dict:\n    \"\"\"Get parameters for this estimator.\n\n    Parameters\n    ----------\n    deep: bool, default False\n        Ignored, exists for sklearn compatibility.\n\n    Returns\n    -------\n    dict\n        Parameter names mapped to their values.\n\n    Note\n    ----\n    Exists for sklearn compatibility.\n    \"\"\"\n    return {\n        \"n_components\": self.n_components,\n        \"n_iter\": self.n_iter,\n        \"alpha\": self.alpha,\n        \"beta\": self.beta,\n    }\n</code></pre>"},{"location":"DirichletMultinomialMixture/#noloox.mixture.DirichletMultinomialMixture.predict","title":"<code>predict(X)</code>","text":"<p>Predicts cluster labels for a set of documents. Mainly exists for compatibility with density estimators in sklearn.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Document-term matrix.</p> required <p>Returns:</p> Type Description <code>array of shape (n_samples,)</code> <p>Cluster label for each document.</p> <p>Raises:</p> Type Description <code>NotFittedException</code> <p>If the model is not fitted, an exception will be raised</p> Source code in <code>noloox/mixture/dmm.py</code> <pre><code>def predict(self, X) -&gt; np.ndarray:\n    \"\"\"Predicts cluster labels for a set of documents. Mainly exists for\n    compatibility with density estimators in sklearn.\n\n    Parameters\n    ----------\n    X: array-like  of shape (n_samples, n_features)\n        Document-term matrix.\n\n    Returns\n    -------\n    array of shape (n_samples,)\n        Cluster label for each document.\n\n    Raises\n    ------\n    NotFittedException\n        If the model is not fitted, an exception will be raised\n    \"\"\"\n    return np.argmax(self.predict_proba(X), axis=1)\n</code></pre>"},{"location":"DirichletMultinomialMixture/#noloox.mixture.DirichletMultinomialMixture.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predicts probabilities for each document belonging to each component.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Document-term matrix.</p> required <p>Returns:</p> Type Description <code>array of shape (n_samples, n_components)</code> <p>Probabilities for each document belonging to each cluster.</p> <p>Raises:</p> Type Description <code>NotFittedException</code> <p>If the model is not fitted, an exception will be raised</p> Source code in <code>noloox/mixture/dmm.py</code> <pre><code>def predict_proba(self, X) -&gt; np.ndarray:\n    \"\"\"Predicts probabilities for each document belonging to each\n    component.\n\n    Parameters\n    ----------\n    X: array-like  of shape (n_samples, n_features)\n        Document-term matrix.\n\n    Returns\n    -------\n    array of shape (n_samples, n_components)\n        Probabilities for each document belonging to each cluster.\n\n    Raises\n    ------\n    NotFittedException\n        If the model is not fitted, an exception will be raised\n    \"\"\"\n    if not hasattr(self, \"_predict_proba\"):\n        raise NotFittedError(\"Model not fitted yet, can't predict probabilities.\")\n    if issparse(X):\n        warnings.warn(\n            \"Sparse arrays are not yet supported. Implicitly converting to dense array.\"\n        )\n        X = np.asarray(X.todense())\n    p = self._predict_proba(X)\n    return np.asarray(p)\n</code></pre>"},{"location":"DirichletMultinomialMixture/#noloox.mixture.DirichletMultinomialMixture.transform","title":"<code>transform(X)</code>","text":"<p>Alias for predict_proba().</p> Source code in <code>noloox/mixture/dmm.py</code> <pre><code>def transform(self, X) -&gt; np.ndarray:\n    \"\"\"Alias for predict_proba().\"\"\"\n    return self.predict_proba(X)\n</code></pre>"},{"location":"Peax/","title":"Peax","text":""},{"location":"Peax/#noloox.cluster.Peax","title":"<code>noloox.cluster.Peax</code>","text":"<p>             Bases: <code>ClusterMixin</code>, <code>BaseEstimator</code></p> <p>Peax clustering model. The model estimates the number of clusters from density peaks, then uses Gaussian Mixtures with fixed means to estimate cluster probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>Optional[int]</code> <p>Random seed to use for fitting gaussian mixture to peaks.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>labels_</code> <code>ndarray of shape (n_samples,)</code> <p>Cluster labels for each point in <code>X</code>.</p> <code>gmm_</code> <code>FixedMeanGaussianMixture</code> <p>The fitted Gaussian mixture model with fixed means.</p> <code>means_</code> <code>ndarray of shape (n_components, 2)</code> <p>Coordinates of detected density peaks used as cluster means.</p> <code>weights_</code> <code>ndarray of shape (n_components,)</code> <p>Final mixture component weights after refitting.</p> <code>classes_</code> <code>ndarray of shape (n_components,)</code> <p>Sorted array of unique cluster labels.</p> <code>density</code> <code>gaussian_kde</code> <p>Kernel density estimator fitted to the input data.</p> Source code in <code>noloox/cluster/peax.py</code> <pre><code>class Peax(ClusterMixin, BaseEstimator):\n    \"\"\"Peax clustering model.\n    The model estimates the number of clusters from density peaks,\n    then uses Gaussian Mixtures with fixed means to estimate cluster\n    probabilities.\n\n    Parameters\n    ----------\n    random_state: int, default None\n        Random seed to use for fitting gaussian mixture to peaks.\n\n    Attributes\n    ----------\n    labels_ : ndarray of shape (n_samples,)\n        Cluster labels for each point in `X`.\n    gmm_ : FixedMeanGaussianMixture\n        The fitted Gaussian mixture model with fixed means.\n    means_ : ndarray of shape (n_components, 2)\n        Coordinates of detected density peaks used as cluster means.\n    weights_ : ndarray of shape (n_components,)\n        Final mixture component weights after refitting.\n    classes_ : ndarray of shape (n_components,)\n        Sorted array of unique cluster labels.\n    density : gaussian_kde\n        Kernel density estimator fitted to the input data.\n    \"\"\"\n\n    def __init__(self, random_state: Optional[int] = None):\n        self.random_state = random_state\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Fit Peax clustering model and cluster datapoints.\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row corresponds to a single data point.\n\n        y: Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        labels: ndarray of shape (n_samples,)\n            Cluster labels for each datapoint.\n        \"\"\"\n        if X.shape[1] &gt; 2:\n            raise ValueError(\n                f\"X has {X.shape[1]} &gt; 2 features. Peax only accepts 2D data.\"\n            )\n        self.X_range = np.min(X), np.max(X)\n        self.density = gaussian_kde(X.T, \"scott\")\n        coord = np.linspace(*self.X_range, num=100)\n        z = []\n        for yval in coord:\n            points = np.stack([coord, np.full(coord.shape, yval)]).T\n            prob = np.exp(self.density.logpdf(points.T))\n            z.append(prob)\n        z = np.stack(z)\n        peaks = detect_peaks(z.T)\n        peak_ind = np.nonzero(peaks)\n        peak_pos = np.stack([coord[peak_ind[0]], coord[peak_ind[1]]]).T\n        weights = self.density.pdf(peak_pos.T)\n        weights = weights / weights.sum()\n        self.gmm_ = FixedMeanGaussianMixture(\n            peak_pos.shape[0],\n            means_init=peak_pos,\n            weights_init=weights,\n            random_state=self.random_state,\n        )\n        self.labels_ = self.gmm_.fit_predict(X)\n        # Checking whether there are close to zero components\n        is_zero = np.isclose(self.gmm_.weights_, 0)\n        n_zero = np.sum(is_zero)\n        if n_zero &gt; 0:\n            print(f\"{n_zero} components have zero weight, removing them and refitting.\")\n        peak_pos = peak_pos[~is_zero]\n        weights = self.gmm_.weights_[~is_zero]\n        weights = weights / weights.sum()\n        self.gmm_ = FixedMeanGaussianMixture(\n            peak_pos.shape[0],\n            means_init=peak_pos,\n            weights_init=weights,\n            random_state=self.random_state,\n        )\n        self.labels_ = self.gmm_.fit_predict(X)\n        self.classes_ = np.sort(np.unique(self.labels_))\n        self.means_ = self.gmm_.means_\n        self.weights_ = self.gmm_.weights_\n        return self.labels_\n\n    def fit(self, X, y=None):\n        \"\"\"Fits clustering model to data.\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row corresponds to a single data point.\n\n        y: Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self: Peax\n            Fitted clustering model.\n        \"\"\"\n        self.fit_predict(X, y)\n        return self\n\n    @property\n    def n_components(self) -&gt; int:\n        \"\"\"Number of clusters found in the data.\"\"\"\n        return self.gmm_.n_components\n\n    def predict_proba(self, X):\n        \"\"\"Evaluate the components' density for each sample.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        resp : array, shape (n_samples, n_components)\n            Density of each Gaussian component for each sample in X.\n        \"\"\"\n        return self.gmm_.predict_proba(X)\n\n    def score_samples(self, X):\n        return self.density.logpdf(X.T)\n\n    def score(self, X):\n        return np.mean(self.score_samples(X))\n</code></pre>"},{"location":"Peax/#noloox.cluster.Peax.n_components","title":"<code>n_components: int</code>  <code>property</code>","text":"<p>Number of clusters found in the data.</p>"},{"location":"Peax/#noloox.cluster.Peax.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits clustering model to data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <code>y</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>Peax</code> <p>Fitted clustering model.</p> Source code in <code>noloox/cluster/peax.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fits clustering model to data.\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row corresponds to a single data point.\n\n    y: Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    self: Peax\n        Fitted clustering model.\n    \"\"\"\n    self.fit_predict(X, y)\n    return self\n</code></pre>"},{"location":"Peax/#noloox.cluster.Peax.fit_predict","title":"<code>fit_predict(X, y=None)</code>","text":"<p>Fit Peax clustering model and cluster datapoints.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <code>y</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray of shape (n_samples,)</code> <p>Cluster labels for each datapoint.</p> Source code in <code>noloox/cluster/peax.py</code> <pre><code>def fit_predict(self, X, y=None):\n    \"\"\"Fit Peax clustering model and cluster datapoints.\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row corresponds to a single data point.\n\n    y: Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    labels: ndarray of shape (n_samples,)\n        Cluster labels for each datapoint.\n    \"\"\"\n    if X.shape[1] &gt; 2:\n        raise ValueError(\n            f\"X has {X.shape[1]} &gt; 2 features. Peax only accepts 2D data.\"\n        )\n    self.X_range = np.min(X), np.max(X)\n    self.density = gaussian_kde(X.T, \"scott\")\n    coord = np.linspace(*self.X_range, num=100)\n    z = []\n    for yval in coord:\n        points = np.stack([coord, np.full(coord.shape, yval)]).T\n        prob = np.exp(self.density.logpdf(points.T))\n        z.append(prob)\n    z = np.stack(z)\n    peaks = detect_peaks(z.T)\n    peak_ind = np.nonzero(peaks)\n    peak_pos = np.stack([coord[peak_ind[0]], coord[peak_ind[1]]]).T\n    weights = self.density.pdf(peak_pos.T)\n    weights = weights / weights.sum()\n    self.gmm_ = FixedMeanGaussianMixture(\n        peak_pos.shape[0],\n        means_init=peak_pos,\n        weights_init=weights,\n        random_state=self.random_state,\n    )\n    self.labels_ = self.gmm_.fit_predict(X)\n    # Checking whether there are close to zero components\n    is_zero = np.isclose(self.gmm_.weights_, 0)\n    n_zero = np.sum(is_zero)\n    if n_zero &gt; 0:\n        print(f\"{n_zero} components have zero weight, removing them and refitting.\")\n    peak_pos = peak_pos[~is_zero]\n    weights = self.gmm_.weights_[~is_zero]\n    weights = weights / weights.sum()\n    self.gmm_ = FixedMeanGaussianMixture(\n        peak_pos.shape[0],\n        means_init=peak_pos,\n        weights_init=weights,\n        random_state=self.random_state,\n    )\n    self.labels_ = self.gmm_.fit_predict(X)\n    self.classes_ = np.sort(np.unique(self.labels_))\n    self.means_ = self.gmm_.means_\n    self.weights_ = self.gmm_.weights_\n    return self.labels_\n</code></pre>"},{"location":"Peax/#noloox.cluster.Peax.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Evaluate the components' density for each sample.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <p>Returns:</p> Name Type Description <code>resp</code> <code>(array, shape(n_samples, n_components))</code> <p>Density of each Gaussian component for each sample in X.</p> Source code in <code>noloox/cluster/peax.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Evaluate the components' density for each sample.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row\n        corresponds to a single data point.\n\n    Returns\n    -------\n    resp : array, shape (n_samples, n_components)\n        Density of each Gaussian component for each sample in X.\n    \"\"\"\n    return self.gmm_.predict_proba(X)\n</code></pre>"},{"location":"SNMF/","title":"SNMF","text":""},{"location":"SNMF/#noloox.decomposition.SNMF","title":"<code>noloox.decomposition.SNMF</code>","text":"<p>             Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Semi-Nonnegative Matrix Factorization. Equivalent to NMF, except the components, and therefore the outcome variables are unbounded. The latent factors are constrained to be nonnegative.</p> <p>Example: <pre><code>import numpy as np\nfrom noloox.decomposition import SNMF\n\nX = np.random.normal(0, 1, size=(200, 50))\nmodel = SNMF(n_components=10)\n\nX_transformed = model.fit_transform(X)\nassert np.all(X_transformed &gt;= 0)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of latent components to discover.</p> required <code>tol</code> <code>float</code> <p>Tolerance for stopping condition.</p> <code>1e-05</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>200</code> <code>progress_bar</code> <code>bool</code> <p>Indicates whether to display a progress bar when fitting.</p> <code>True</code> <code>random_state</code> <code>Optional[int]</code> <p>Used for model intialization with KMeans.</p> <code>None</code> <code>sparsity</code> <code>float</code> <p>L1 penalty. Higher values result in a stricter clustering.</p> <code>0.0</code> <p>Attributes:</p> Name Type Description <code>components_</code> <code>ndarray of shape (n_components, n_features)</code> <p>Factorization matrix, sometimes called \u2018dictionary\u2019. Unconstrained.</p> <code>n_iter_</code> <code>int</code> <p>Acutal number of iterations.</p> <code>reconstruction_err_</code> <code>float</code> <p>Reconstruction error of the model at the last iteration.</p> Source code in <code>noloox/decomposition/snmf.py</code> <pre><code>class SNMF(TransformerMixin, BaseEstimator):\n    \"\"\"Semi-Nonnegative Matrix Factorization.\n    Equivalent to NMF, except the components, and therefore the outcome variables are unbounded.\n    The latent factors are constrained to be nonnegative.\n\n    Example:\n    ```python\n    import numpy as np\n    from noloox.decomposition import SNMF\n\n    X = np.random.normal(0, 1, size=(200, 50))\n    model = SNMF(n_components=10)\n\n    X_transformed = model.fit_transform(X)\n    assert np.all(X_transformed &gt;= 0)\n    ```\n\n    Parameters\n    ----------\n    n_components: int\n        Number of latent components to discover.\n    tol: float, default=1e-5\n        Tolerance for stopping condition.\n    max_iter: int, default=200\n        Maximum number of iterations.\n    progress_bar: bool, default=True\n        Indicates whether to display a progress bar when fitting.\n    random_state: int, default=None\n        Used for model intialization with KMeans.\n    sparsity: float, default=0.0\n        L1 penalty. Higher values result in a stricter clustering.\n\n    Attributes\n    ----------\n    components_: ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called \u2018dictionary\u2019.\n        Unconstrained.\n    n_iter_: int\n        Acutal number of iterations.\n    reconstruction_err_: float\n        Reconstruction error of the model at the last iteration.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        tol: float = 1e-5,\n        max_iter: int = 200,\n        progress_bar: bool = True,\n        random_state: Optional[int] = None,\n        sparsity: float = 0.0,\n        verbose: bool = False,\n    ):\n        self.n_components = n_components\n        self.tol = tol\n        self.max_iter = max_iter\n        self.progress_bar = progress_bar\n        self.random_state = random_state\n        self.sparsity = sparsity\n        self.verbose = verbose\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Learn an SNMF model for the data X and returns the transformed data.\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            Datapoints to factor.\n        y: Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data. Strictily nonnegative.\n        \"\"\"\n        G = init_G(X.T, self.n_components, random_state=self.random_state)\n        F = update_F(X.T, G)\n        error_at_init = rec_err(X.T, F, G)\n        prev_error = error_at_init\n        _step = partial(step, sparsity=self.sparsity, X=X)\n        for i in trange(\n            self.max_iter,\n            desc=\"Iterative updates.\",\n            disable=not self.progress_bar,\n        ):\n            G, F, error = _step(G, F)\n            difference = prev_error - error\n            if (error &lt; error_at_init) and (\n                (prev_error - error) / error_at_init\n            ) &lt; self.tol:\n                if self.verbose:\n                    print(f\"Converged after {i} iterations\")\n                self.n_iter_ = i\n                break\n            prev_error = error\n            if self.verbose:\n                print(\n                    f\"Iteration: {i}, Error: {error}, init_error: {error_at_init}, difference from previous: {difference}\"\n                )\n        else:\n            warnings.warn(\"SNMF did not converge, try specifying a higher max_iter.\")\n        self.components_ = np.array(F.T)\n        self.reconstruction_err_ = error\n        self.n_iter_ = i\n        return np.array(G)\n\n    def fit(self, X, y=None):\n        \"\"\"Learn an SNMF model for the data X.\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            Datapoints to factor.\n        y: Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self: SNMF\n            Fitted model.\n        \"\"\"\n        self.fit_transform(X, y)\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform the data X according to the fitted SNMF model.\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            Datapoints to transform.\n\n        Returns\n        -------\n        W: ndarray of shape (n_samples, n_components)\n            Nonnegative latent sources.\n        \"\"\"\n        G = jnp.maximum(X @ jnp.linalg.pinv(self.components_), 0)\n        return np.array(G)\n\n    def inverse_transform(self, X):\n        \"\"\"Transform data back to its original space.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_components)\n            Transformed data matrix.\n\n        Returns\n        -------\n        X_original : ndarray of shape (n_samples, n_features)\n            Returns a data matrix of the original shape.\n        \"\"\"\n        return X @ self.components_\n</code></pre>"},{"location":"SNMF/#noloox.decomposition.SNMF.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Learn an SNMF model for the data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Datapoints to factor.</p> required <code>y</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>SNMF</code> <p>Fitted model.</p> Source code in <code>noloox/decomposition/snmf.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Learn an SNMF model for the data X.\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        Datapoints to factor.\n    y: Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    self: SNMF\n        Fitted model.\n    \"\"\"\n    self.fit_transform(X, y)\n    return self\n</code></pre>"},{"location":"SNMF/#noloox.decomposition.SNMF.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Learn an SNMF model for the data X and returns the transformed data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Datapoints to factor.</p> required <code>y</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>W</code> <code>ndarray of shape (n_samples, n_components)</code> <p>Transformed data. Strictily nonnegative.</p> Source code in <code>noloox/decomposition/snmf.py</code> <pre><code>def fit_transform(self, X, y=None):\n    \"\"\"Learn an SNMF model for the data X and returns the transformed data.\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        Datapoints to factor.\n    y: Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    W : ndarray of shape (n_samples, n_components)\n        Transformed data. Strictily nonnegative.\n    \"\"\"\n    G = init_G(X.T, self.n_components, random_state=self.random_state)\n    F = update_F(X.T, G)\n    error_at_init = rec_err(X.T, F, G)\n    prev_error = error_at_init\n    _step = partial(step, sparsity=self.sparsity, X=X)\n    for i in trange(\n        self.max_iter,\n        desc=\"Iterative updates.\",\n        disable=not self.progress_bar,\n    ):\n        G, F, error = _step(G, F)\n        difference = prev_error - error\n        if (error &lt; error_at_init) and (\n            (prev_error - error) / error_at_init\n        ) &lt; self.tol:\n            if self.verbose:\n                print(f\"Converged after {i} iterations\")\n            self.n_iter_ = i\n            break\n        prev_error = error\n        if self.verbose:\n            print(\n                f\"Iteration: {i}, Error: {error}, init_error: {error_at_init}, difference from previous: {difference}\"\n            )\n    else:\n        warnings.warn(\"SNMF did not converge, try specifying a higher max_iter.\")\n    self.components_ = np.array(F.T)\n    self.reconstruction_err_ = error\n    self.n_iter_ = i\n    return np.array(G)\n</code></pre>"},{"location":"SNMF/#noloox.decomposition.SNMF.inverse_transform","title":"<code>inverse_transform(X)</code>","text":"<p>Transform data back to its original space.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of shape (n_samples, n_components)</code> <p>Transformed data matrix.</p> required <p>Returns:</p> Name Type Description <code>X_original</code> <code>ndarray of shape (n_samples, n_features)</code> <p>Returns a data matrix of the original shape.</p> Source code in <code>noloox/decomposition/snmf.py</code> <pre><code>def inverse_transform(self, X):\n    \"\"\"Transform data back to its original space.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_components)\n        Transformed data matrix.\n\n    Returns\n    -------\n    X_original : ndarray of shape (n_samples, n_features)\n        Returns a data matrix of the original shape.\n    \"\"\"\n    return X @ self.components_\n</code></pre>"},{"location":"SNMF/#noloox.decomposition.SNMF.transform","title":"<code>transform(X)</code>","text":"<p>Transform the data X according to the fitted SNMF model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Datapoints to transform.</p> required <p>Returns:</p> Name Type Description <code>W</code> <code>ndarray of shape (n_samples, n_components)</code> <p>Nonnegative latent sources.</p> Source code in <code>noloox/decomposition/snmf.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform the data X according to the fitted SNMF model.\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        Datapoints to transform.\n\n    Returns\n    -------\n    W: ndarray of shape (n_samples, n_components)\n        Nonnegative latent sources.\n    \"\"\"\n    G = jnp.maximum(X @ jnp.linalg.pinv(self.components_), 0)\n    return np.array(G)\n</code></pre>"},{"location":"StudentsTMixture/","title":"StudentsTMixture","text":""},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture","title":"<code>noloox.mixture.StudentsTMixture</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClusterMixin</code>, <code>DensityMixin</code></p> <p>Student's T Mixture Model. This class allows you to estimate a mixture of multivariate t-distributions over your data.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>The number of mixture components.</p> required <code>tol</code> <code>float</code> <p>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</p> <code>1e-05</code> <code>reg_covar</code> <code>float</code> <p>Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.</p> <code>1e-06</code> <code>max_iter</code> <code>int</code> <p>The number of EM iterations to perform.</p> <code>1000</code> <code>df</code> <p>Degrees of freedom for the t-Distributions.</p> <code>4.0</code> <code>random_state</code> <p>Random state for reproducibility.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>weights_</code> <code>array-like of shape (n_components,)</code> <p>The weights of each mixture components.</p> <code>means_</code> <code>array-like of shape (n_components, n_features)</code> <p>The mean of each mixture component.</p> <code>n_iter_</code> <code>int</code> <p>Number of step used by the best fit of EM to reach the convergence.</p> <code>converged_</code> <code>bool</code> <p>True when convergence of the best fit of EM was reached, False otherwise.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>class StudentsTMixture(BaseEstimator, ClusterMixin, DensityMixin):\n    \"\"\"Student's T Mixture Model.\n    This class allows you to estimate a mixture of multivariate t-distributions over your data.\n\n    Parameters\n    ----------\n    n_components: int\n        The number of mixture components.\n    tol: float, default=1e-5\n        The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.\n    reg_covar: float, default=1e-6\n        Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.\n    max_iter: int, default=1000\n        The number of EM iterations to perform.\n    df: float, default=4.0\n        Degrees of freedom for the t-Distributions.\n    random_state: int, default=None\n        Random state for reproducibility.\n\n    Attributes\n    ----------\n    weights_: array-like of shape (n_components,)\n        The weights of each mixture components.\n    means_: array-like of shape (n_components, n_features)\n        The mean of each mixture component.\n    n_iter_: int\n        Number of step used by the best fit of EM to reach the convergence.\n    converged_: bool\n        True when convergence of the best fit of EM was reached, False otherwise.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        tol: float = 1e-5,\n        reg_covar: float = 1e-06,\n        max_iter: int = 1000,\n        df=4.0,\n        random_state=None,\n    ):\n        super().__init__()\n        self.start_df_ = float(df)\n        self.n_components = n_components\n        self.tol = tol\n        self.df = df\n        self.reg_covar = reg_covar\n        self.max_iter = max_iter\n        self.random_state = random_state\n\n    def _init_params(self, X):\n        loc_ = (\n            KMeans(self.n_components, random_state=self.random_state)\n            .fit(X)\n            .cluster_centers_\n        )\n        mix_weights_ = np.ones(self.n_components) / self.n_components\n        default_scale_matrix = np.cov(X, rowvar=False)\n        default_scale_matrix.flat[:: X.shape[1] + 1] += self.reg_covar\n        if len(default_scale_matrix.shape) &lt; 2:\n            default_scale_matrix = default_scale_matrix.reshape(-1, 1)\n        scale_ = np.stack(\n            [default_scale_matrix for i in range(self.n_components)], axis=-1\n        )\n        scale_cholesky_ = [\n            np.linalg.cholesky(scale_[:, :, i]) for i in range(self.n_components)\n        ]\n        scale_cholesky_ = np.stack(scale_cholesky_, axis=-1)\n        return loc_, scale_, mix_weights_, scale_cholesky_\n\n    @staticmethod\n    def e_step(X, df_, loc_, scale_cholesky_, mix_weights_, sq_maha_dist):\n        return _e_step(X, df_, loc_, scale_cholesky_, mix_weights_, sq_maha_dist)\n\n    @staticmethod\n    def m_step(X, resp, E_gamma, scale_, scale_cholesky_, df_, reg_covar):\n        return _m_step(X, resp, E_gamma, scale_, scale_cholesky_, df_, reg_covar)\n\n    def fit(self, X, y=None):\n        \"\"\"Estimate model parameters with the EM algorithm.\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row corresponds to a single data point.\n\n        y: Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self: StudentsTMixture\n            The fitted mixture.\n        \"\"\"\n        self.df_ = np.full((self.n_components), self.df, dtype=np.float64)\n        loc_, scale_, mix_weights_, scale_cholesky_ = self._init_params(X)\n        lower_bound = -np.inf\n        sq_maha_dist = np.empty((X.shape[0], self.n_components))\n        self.converged_ = True\n\n        @jax.jit\n        def step(loc_, mix_weights_, scale_cholesky_, scale_, sq_maha_dist):\n            resp, E_gamma, current_bound = self.e_step(\n                X, self.df_, loc_, scale_cholesky_, mix_weights_, sq_maha_dist\n            )\n            mix_weights_, loc_, scale_, scale_cholesky_ = self.m_step(\n                X,\n                resp,\n                E_gamma,\n                scale_,\n                scale_cholesky_,\n                self.df_,\n                self.reg_covar,\n            )\n            return (\n                loc_,\n                mix_weights_,\n                scale_cholesky_,\n                scale_,\n                sq_maha_dist,\n                current_bound,\n            )\n\n        for iter in trange(self.max_iter, desc=\"Running EM\"):\n            loc_, mix_weights_, scale_cholesky_, scale_, sq_maha_dist, current_bound = (\n                step(\n                    loc_,\n                    mix_weights_,\n                    scale_cholesky_,\n                    scale_,\n                    sq_maha_dist,\n                )\n            )\n            change = current_bound - lower_bound\n            if abs(change) &lt; self.tol:\n                break\n            lower_bound = current_bound\n        else:\n            self.converged_ = False\n            warnings.warn(\n                \"tDistributionMixture did not converge, consider increasing the number of iterations.\"\n            )\n        self.n_iter_ = iter\n        self.weights_ = mix_weights_\n        self.means_ = loc_\n        self.scale_cholesky_ = scale_cholesky_\n        self.scale_ = scale_\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Evaluate the components' density for each sample.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        resp : array, shape (n_samples, n_components)\n            Density of each Student's T component for each sample in X.\n        \"\"\"\n        sq_maha_dist = sq_maha_distance(X, self.means_, self.scale_cholesky_)\n        loglik = get_loglikelihood(\n            X, sq_maha_dist, self.df_, self.scale_cholesky_, self.weights_\n        )\n        weighted_loglik = loglik + np.log(self.weights_)[np.newaxis, :]\n        with np.errstate(under=\"ignore\"):\n            loglik = weighted_loglik - logsumexp(weighted_loglik, axis=1)[:, np.newaxis]\n        return np.exp(loglik)\n\n    def predict(self, X):\n        \"\"\"Predict the labels for the data samples in X using trained model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        labels : array, shape (n_samples,)\n            Component labels.\n        \"\"\"\n        return np.argmax(self.predict_proba(X), axis=1)\n\n    def fit_predict(self, X):\n        \"\"\"Estimate model parameters using X and predict the labels for X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        labels : array, shape (n_samples,)\n            Component labels.\n        \"\"\"\n        return self.fit(X).predict(X)\n\n    def aic(self, X):\n        \"\"\"Akaike information criterion for the current model on the input X.\n\n        Parameters\n        ----------\n        X: array of shape (n_samples, n_dimensions)\n            The input samples.\n\n        Returns\n        -------\n        aic: float\n            The lower the better.\n        \"\"\"\n        self.check_model()\n        x = self.check_inputs(X)\n        n_params = self.get_num_parameters()\n        score = self.score(x, perform_model_checks=False)\n        return 2 * n_params - 2 * score * X.shape[0]\n\n    def bic(self, X):\n        \"\"\"Bayesian information criterion for the current model on the input X.\n\n        Parameters\n        ----------\n        X: array of shape (n_samples, n_dimensions)\n            The input samples.\n\n        Returns\n        -------\n        bic: float\n            The lower the better.\n        \"\"\"\n        self.check_model()\n        x = self.check_inputs(X)\n        score = self.score(x, perform_model_checks=False)\n        n_params = self.get_num_parameters()\n        return n_params * np.log(x.shape[0]) - 2 * score * x.shape[0]\n\n    def score_samples(self, X):\n        \"\"\"Compute the log-likelihood of each sample.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        log_prob : array, shape (n_samples,)\n            Log-likelihood of each sample in `X` under the current model.\n        \"\"\"\n        sq_maha_dist = sq_maha_distance(X, self.means_, self.scale_cholesky_)\n        loglik = get_loglikelihood(\n            X, sq_maha_dist, self.df_, self.scale_cholesky_, self.weights_\n        )\n        weighted_loglik = loglik + np.log(self.weights_)[np.newaxis, :]\n        return logsumexp(weighted_loglik, axis=1)\n\n    def score(self, X, y=None):\n        \"\"\"Compute the per-sample average log-likelihood of the given data X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_dimensions)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        log_likelihood : float\n            Log-likelihood of `X` under the Gaussian mixture model.\n        \"\"\"\n        return np.mean(self.score_samples(X))\n</code></pre>"},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture.aic","title":"<code>aic(X)</code>","text":"<p>Akaike information criterion for the current model on the input X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input samples.</p> required <p>Returns:</p> Name Type Description <code>aic</code> <code>float</code> <p>The lower the better.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>def aic(self, X):\n    \"\"\"Akaike information criterion for the current model on the input X.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_dimensions)\n        The input samples.\n\n    Returns\n    -------\n    aic: float\n        The lower the better.\n    \"\"\"\n    self.check_model()\n    x = self.check_inputs(X)\n    n_params = self.get_num_parameters()\n    score = self.score(x, perform_model_checks=False)\n    return 2 * n_params - 2 * score * X.shape[0]\n</code></pre>"},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture.bic","title":"<code>bic(X)</code>","text":"<p>Bayesian information criterion for the current model on the input X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input samples.</p> required <p>Returns:</p> Name Type Description <code>bic</code> <code>float</code> <p>The lower the better.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>def bic(self, X):\n    \"\"\"Bayesian information criterion for the current model on the input X.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_dimensions)\n        The input samples.\n\n    Returns\n    -------\n    bic: float\n        The lower the better.\n    \"\"\"\n    self.check_model()\n    x = self.check_inputs(X)\n    score = self.score(x, perform_model_checks=False)\n    n_params = self.get_num_parameters()\n    return n_params * np.log(x.shape[0]) - 2 * score * x.shape[0]\n</code></pre>"},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Estimate model parameters with the EM algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <code>y</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>StudentsTMixture</code> <p>The fitted mixture.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Estimate model parameters with the EM algorithm.\n\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row corresponds to a single data point.\n\n    y: Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    self: StudentsTMixture\n        The fitted mixture.\n    \"\"\"\n    self.df_ = np.full((self.n_components), self.df, dtype=np.float64)\n    loc_, scale_, mix_weights_, scale_cholesky_ = self._init_params(X)\n    lower_bound = -np.inf\n    sq_maha_dist = np.empty((X.shape[0], self.n_components))\n    self.converged_ = True\n\n    @jax.jit\n    def step(loc_, mix_weights_, scale_cholesky_, scale_, sq_maha_dist):\n        resp, E_gamma, current_bound = self.e_step(\n            X, self.df_, loc_, scale_cholesky_, mix_weights_, sq_maha_dist\n        )\n        mix_weights_, loc_, scale_, scale_cholesky_ = self.m_step(\n            X,\n            resp,\n            E_gamma,\n            scale_,\n            scale_cholesky_,\n            self.df_,\n            self.reg_covar,\n        )\n        return (\n            loc_,\n            mix_weights_,\n            scale_cholesky_,\n            scale_,\n            sq_maha_dist,\n            current_bound,\n        )\n\n    for iter in trange(self.max_iter, desc=\"Running EM\"):\n        loc_, mix_weights_, scale_cholesky_, scale_, sq_maha_dist, current_bound = (\n            step(\n                loc_,\n                mix_weights_,\n                scale_cholesky_,\n                scale_,\n                sq_maha_dist,\n            )\n        )\n        change = current_bound - lower_bound\n        if abs(change) &lt; self.tol:\n            break\n        lower_bound = current_bound\n    else:\n        self.converged_ = False\n        warnings.warn(\n            \"tDistributionMixture did not converge, consider increasing the number of iterations.\"\n        )\n    self.n_iter_ = iter\n    self.weights_ = mix_weights_\n    self.means_ = loc_\n    self.scale_cholesky_ = scale_cholesky_\n    self.scale_ = scale_\n    return self\n</code></pre>"},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture.fit_predict","title":"<code>fit_predict(X)</code>","text":"<p>Estimate model parameters using X and predict the labels for X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency by convention.</p> required <p>Returns:</p> Name Type Description <code>labels</code> <code>(array, shape(n_samples))</code> <p>Component labels.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>def fit_predict(self, X):\n    \"\"\"Estimate model parameters using X and predict the labels for X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row\n        corresponds to a single data point.\n\n    y : Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    labels : array, shape (n_samples,)\n        Component labels.\n    \"\"\"\n    return self.fit(X).predict(X)\n</code></pre>"},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture.predict","title":"<code>predict(X)</code>","text":"<p>Predict the labels for the data samples in X using trained model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <p>Returns:</p> Name Type Description <code>labels</code> <code>(array, shape(n_samples))</code> <p>Component labels.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict the labels for the data samples in X using trained model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row\n        corresponds to a single data point.\n\n    Returns\n    -------\n    labels : array, shape (n_samples,)\n        Component labels.\n    \"\"\"\n    return np.argmax(self.predict_proba(X), axis=1)\n</code></pre>"},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Evaluate the components' density for each sample.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <p>Returns:</p> Name Type Description <code>resp</code> <code>(array, shape(n_samples, n_components))</code> <p>Density of each Student's T component for each sample in X.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Evaluate the components' density for each sample.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row\n        corresponds to a single data point.\n\n    Returns\n    -------\n    resp : array, shape (n_samples, n_components)\n        Density of each Student's T component for each sample in X.\n    \"\"\"\n    sq_maha_dist = sq_maha_distance(X, self.means_, self.scale_cholesky_)\n    loglik = get_loglikelihood(\n        X, sq_maha_dist, self.df_, self.scale_cholesky_, self.weights_\n    )\n    weighted_loglik = loglik + np.log(self.weights_)[np.newaxis, :]\n    with np.errstate(under=\"ignore\"):\n        loglik = weighted_loglik - logsumexp(weighted_loglik, axis=1)[:, np.newaxis]\n    return np.exp(loglik)\n</code></pre>"},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture.score","title":"<code>score(X, y=None)</code>","text":"<p>Compute the per-sample average log-likelihood of the given data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_dimensions)</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>log_likelihood</code> <code>float</code> <p>Log-likelihood of <code>X</code> under the Gaussian mixture model.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>def score(self, X, y=None):\n    \"\"\"Compute the per-sample average log-likelihood of the given data X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_dimensions)\n        List of n_features-dimensional data points. Each row\n        corresponds to a single data point.\n\n    y : Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    log_likelihood : float\n        Log-likelihood of `X` under the Gaussian mixture model.\n    \"\"\"\n    return np.mean(self.score_samples(X))\n</code></pre>"},{"location":"StudentsTMixture/#noloox.mixture.StudentsTMixture.score_samples","title":"<code>score_samples(X)</code>","text":"<p>Compute the log-likelihood of each sample.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>List of n_features-dimensional data points. Each row corresponds to a single data point.</p> required <p>Returns:</p> Name Type Description <code>log_prob</code> <code>(array, shape(n_samples))</code> <p>Log-likelihood of each sample in <code>X</code> under the current model.</p> Source code in <code>noloox/mixture/tmm.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"Compute the log-likelihood of each sample.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row\n        corresponds to a single data point.\n\n    Returns\n    -------\n    log_prob : array, shape (n_samples,)\n        Log-likelihood of each sample in `X` under the current model.\n    \"\"\"\n    sq_maha_dist = sq_maha_distance(X, self.means_, self.scale_cholesky_)\n    loglik = get_loglikelihood(\n        X, sq_maha_dist, self.df_, self.scale_cholesky_, self.weights_\n    )\n    weighted_loglik = loglik + np.log(self.weights_)[np.newaxis, :]\n    return logsumexp(weighted_loglik, axis=1)\n</code></pre>"},{"location":"WNMF/","title":"WNMF","text":""},{"location":"WNMF/#noloox.decomposition.WNMF","title":"<code>noloox.decomposition.WNMF</code>","text":"<p>             Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Weighted Nonnegative Matrix Factorization (WNMF).</p> <p>WNMF factorizes a nonnegative data matrix \\(X\\) into two nonnegative matrices \\(W\\) and \\(H\\) such that</p> <p>\\(X \\approx W H\\)</p> <p>but introduces per-entry weights given by <code>y</code>. The weights modify the update rules so that reconstruction errors on entries with higher weights contribute more strongly to the objective.</p> <p>The optimization uses multiplicative updates for both factors :math:<code>U</code> and :math:<code>V</code> following a weighted Euclidean (\u03b2=2) divergence objective.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of latent components to use.</p> required <code>max_iter</code> <code>int</code> <p>Maximum number of iterations before stopping.</p> <code>200</code> <code>tol</code> <code>float</code> <p>Tolerance for early stopping. The algorithm checks every 10 iterations whether the reconstruction error has ceased improving.</p> <code>1e-4</code> <code>random_state</code> <code>int or None</code> <p>Seed for random initialization of the factors.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>components_</code> <code>ndarray of shape (n_components, n_features)</code> <p>The learned basis matrix :math:<code>U^T</code>.</p> References <p>Y. -D. Kim and S. Choi, \"Weighted nonnegative matrix factorization,\" 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, Taipei, Taiwan, 2009, pp. 1541-1544, doi: 10.1109/ICASSP.2009.4959890. keywords: {Matrix decomposition;Least squares approximation;Collaboration;Convergence;Least squares methods;Data analysis;Computer science;Singular value decomposition;Feature extraction;Spectrogram;Alternating nonnegative least squares;collaborative prediction;generalized EM;nonnegative matrix factorization;weighted low-rank approximation},</p> Source code in <code>noloox/decomposition/wnmf.py</code> <pre><code>class WNMF(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Weighted Nonnegative Matrix Factorization (WNMF).\n\n    WNMF factorizes a nonnegative data matrix $X$ into two\n    nonnegative matrices $W$ and $H$ such that\n\n    $X \\\\approx W H$\n\n    but introduces per-entry weights given by `y`.\n    The weights modify the update rules so that reconstruction errors on\n    entries with higher weights contribute more strongly to the objective.\n\n    The optimization uses multiplicative updates for both factors\n    :math:`U` and :math:`V` following a weighted Euclidean (\u03b2=2)\n    divergence objective.\n\n    Parameters\n    ----------\n    n_components : int\n        Number of latent components to use.\n    max_iter : int, default=200\n        Maximum number of iterations before stopping.\n    tol : float, default=1e-4\n        Tolerance for early stopping. The algorithm checks every 10 iterations\n        whether the reconstruction error has ceased improving.\n    random_state : int or None, default=None\n        Seed for random initialization of the factors.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The learned basis matrix :math:`U^T`.\n\n    References\n    ----------\n    Y. -D. Kim and S. Choi, \"Weighted nonnegative matrix factorization,\" 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, Taipei, Taiwan, 2009, pp. 1541-1544, doi: 10.1109/ICASSP.2009.4959890. keywords: {Matrix decomposition;Least squares approximation;Collaboration;Convergence;Least squares methods;Data analysis;Computer science;Singular value decomposition;Feature extraction;Spectrogram;Alternating nonnegative least squares;collaborative prediction;generalized EM;nonnegative matrix factorization;weighted low-rank approximation},\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        max_iter: int = 200,\n        tol: float = 1e-4,\n        random_state: Optional[int] = None,\n    ):\n        self.n_components = n_components\n        self.max_iter = max_iter\n        self.tol = tol\n        self.random_state = random_state\n\n    def fit_transform(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"Fit the WNMF model to data `X` with weights `y` and return the\n        transformed representation.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Nonnegative data matrix to factorize.\n\n        y : ndarray of shape (n_samples,)\n            Weights applied to each entry of `X`.\n            Larger values increase the importance of corresponding entries.\n\n        Returns\n        -------\n        X_transformed : ndarray of shape (n_samples, n_components)\n            The learned encoding matrix $V^T$ of the data.\n        \"\"\"\n        X_transformed, components = _initialize_nmf(\n            X, self.n_components, random_state=self.random_state\n        )\n        U = components.T\n        V = X_transformed.T\n        weighted_A = X.T * y  # .T\n        prev_error = np.inf\n        for i in range(0, self.max_iter):\n            # Update V\n            numerator = safe_sparse_dot(U.T, weighted_A)\n            denominator = np.linalg.multi_dot((U.T, U, V * y))\n            denominator[denominator &lt;= 0] = EPSILON\n            delta = numerator\n            delta /= denominator\n            delta[np.isinf(delta) &amp; (V == 0)] = 0\n            V *= delta\n            # Update U\n            numerator = safe_sparse_dot(weighted_A, V.T)\n            denominator = np.linalg.multi_dot((U, V * y, V.T))\n            denominator[denominator &lt;= 0] = EPSILON\n            delta = numerator\n            delta /= denominator\n            delta[np.isinf(delta) &amp; (U == 0)] = 0\n            U *= delta\n            if (self.tol &gt; 0) and (i % 10 == 0):\n                error = _beta_divergence(X, V.T, U.T, 2)\n                if (error - prev_error) &gt; self.tol:\n                    break\n                prev_error = error\n        self.components_ = U.T\n        X_transformed = V.T\n        return X_transformed\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the WNMF model to data `X` with weights `y`.\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Nonnegative data matrix to factorize.\n\n        y : ndarray of shape (n_samples,)\n            Weights applied to each entry of `X`.\n            Larger values increase the importance of corresponding entries.\n\n        Returns\n        -------\n        self\n            Fitted WNMF model.\n        \"\"\"\n        self.fit_transform(X, y)\n        return self\n\n    def transform(self, X: np.ndarray):\n        \"\"\"Transform new data according to the fitted WNMF components.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data to transform.\n\n        Returns\n        -------\n        X_transformed : ndarray of shape (n_samples, n_components)\n            Nonnegative encoded representation of the input.\n        \"\"\"\n        X_transformed = np.maximum(X @ np.linalg.pinv(self.components_), 0)\n        return np.array(X_transformed)\n\n    def inverse_transform(self, X):\n        \"\"\"Transform data back to its original space.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_components)\n            Transformed data matrix.\n\n        Returns\n        -------\n        X_original : ndarray of shape (n_samples, n_features)\n            Returns a data matrix of the original shape.\n        \"\"\"\n        return X @ self.components_\n</code></pre>"},{"location":"WNMF/#noloox.decomposition.WNMF.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the WNMF model to data <code>X</code> with weights <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of shape (n_samples, n_features)</code> <p>Nonnegative data matrix to factorize.</p> required <code>y</code> <code>ndarray of shape (n_samples,)</code> <p>Weights applied to each entry of <code>X</code>. Larger values increase the importance of corresponding entries.</p> <code>None</code> <p>Returns:</p> Type Description <code>self</code> <p>Fitted WNMF model.</p> Source code in <code>noloox/decomposition/wnmf.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the WNMF model to data `X` with weights `y`.\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Nonnegative data matrix to factorize.\n\n    y : ndarray of shape (n_samples,)\n        Weights applied to each entry of `X`.\n        Larger values increase the importance of corresponding entries.\n\n    Returns\n    -------\n    self\n        Fitted WNMF model.\n    \"\"\"\n    self.fit_transform(X, y)\n    return self\n</code></pre>"},{"location":"WNMF/#noloox.decomposition.WNMF.fit_transform","title":"<code>fit_transform(X, y)</code>","text":"<p>Fit the WNMF model to data <code>X</code> with weights <code>y</code> and return the transformed representation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of shape (n_samples, n_features)</code> <p>Nonnegative data matrix to factorize.</p> required <code>y</code> <code>ndarray of shape (n_samples,)</code> <p>Weights applied to each entry of <code>X</code>. Larger values increase the importance of corresponding entries.</p> required <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>ndarray of shape (n_samples, n_components)</code> <p>The learned encoding matrix \\(V^T\\) of the data.</p> Source code in <code>noloox/decomposition/wnmf.py</code> <pre><code>def fit_transform(self, X: np.ndarray, y: np.ndarray):\n    \"\"\"Fit the WNMF model to data `X` with weights `y` and return the\n    transformed representation.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Nonnegative data matrix to factorize.\n\n    y : ndarray of shape (n_samples,)\n        Weights applied to each entry of `X`.\n        Larger values increase the importance of corresponding entries.\n\n    Returns\n    -------\n    X_transformed : ndarray of shape (n_samples, n_components)\n        The learned encoding matrix $V^T$ of the data.\n    \"\"\"\n    X_transformed, components = _initialize_nmf(\n        X, self.n_components, random_state=self.random_state\n    )\n    U = components.T\n    V = X_transformed.T\n    weighted_A = X.T * y  # .T\n    prev_error = np.inf\n    for i in range(0, self.max_iter):\n        # Update V\n        numerator = safe_sparse_dot(U.T, weighted_A)\n        denominator = np.linalg.multi_dot((U.T, U, V * y))\n        denominator[denominator &lt;= 0] = EPSILON\n        delta = numerator\n        delta /= denominator\n        delta[np.isinf(delta) &amp; (V == 0)] = 0\n        V *= delta\n        # Update U\n        numerator = safe_sparse_dot(weighted_A, V.T)\n        denominator = np.linalg.multi_dot((U, V * y, V.T))\n        denominator[denominator &lt;= 0] = EPSILON\n        delta = numerator\n        delta /= denominator\n        delta[np.isinf(delta) &amp; (U == 0)] = 0\n        U *= delta\n        if (self.tol &gt; 0) and (i % 10 == 0):\n            error = _beta_divergence(X, V.T, U.T, 2)\n            if (error - prev_error) &gt; self.tol:\n                break\n            prev_error = error\n    self.components_ = U.T\n    X_transformed = V.T\n    return X_transformed\n</code></pre>"},{"location":"WNMF/#noloox.decomposition.WNMF.inverse_transform","title":"<code>inverse_transform(X)</code>","text":"<p>Transform data back to its original space.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of shape (n_samples, n_components)</code> <p>Transformed data matrix.</p> required <p>Returns:</p> Name Type Description <code>X_original</code> <code>ndarray of shape (n_samples, n_features)</code> <p>Returns a data matrix of the original shape.</p> Source code in <code>noloox/decomposition/wnmf.py</code> <pre><code>def inverse_transform(self, X):\n    \"\"\"Transform data back to its original space.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_components)\n        Transformed data matrix.\n\n    Returns\n    -------\n    X_original : ndarray of shape (n_samples, n_features)\n        Returns a data matrix of the original shape.\n    \"\"\"\n    return X @ self.components_\n</code></pre>"},{"location":"WNMF/#noloox.decomposition.WNMF.transform","title":"<code>transform(X)</code>","text":"<p>Transform new data according to the fitted WNMF components.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of shape (n_samples, n_features)</code> <p>The data to transform.</p> required <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>ndarray of shape (n_samples, n_components)</code> <p>Nonnegative encoded representation of the input.</p> Source code in <code>noloox/decomposition/wnmf.py</code> <pre><code>def transform(self, X: np.ndarray):\n    \"\"\"Transform new data according to the fitted WNMF components.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        The data to transform.\n\n    Returns\n    -------\n    X_transformed : ndarray of shape (n_samples, n_components)\n        Nonnegative encoded representation of the input.\n    \"\"\"\n    X_transformed = np.maximum(X @ np.linalg.pinv(self.components_), 0)\n    return np.array(X_transformed)\n</code></pre>"},{"location":"count_clustering/","title":"Clustering Count Data","text":"<p>While algorithms like KMeans and Gaussian Mixtures might be perfectly suitable for clustering continuous and normally distributed data, they are not an appropriate choice for clustering multivariate count data.</p> <p>In this example we will generate synthetic count data with underlying true clusters, and then look at how to recover these clusters completely unsupervised using <code>DirichletMultinomialMixture</code>.</p>"},{"location":"count_clustering/#simulating-data","title":"Simulating data","text":"<p>In our example we will generate data about where students from different high-schools in a region ended up after having graduated from the school. Let's say that we have 200 schools, and each of the schools has thirty students. Every student that graduates has some probability of becoming and engineer, and academic, a doctor, a lawyer, an artist, a musician, a skilled worker, or something else.</p> <p>We will assume that there are three underlying clusters in this data, and will use this assumption to generate stochastic data from these three types of schools.</p> <ol> <li>20% of schools have an academic orientation, where most graduates become engineers, academics, doctors or lawyers.</li> <li>10% of schools have an art orientation, where most graduates become artists and musicians with some probability of becoming an academic or something else.</li> <li>70% of schools have a technical orientation, where most graduates go on to be engineers or skilled workers.</li> </ol> <p>We will simulate the number of graduates going into any one of the listed professions from each school using NumPy.</p> <pre><code>import numpy as np\n\ngenerator = np.random.default_rng(42)\n\ncolumns = [\"engineers\", \"academics\", \"doctors\", \"lawyers\", \"artists\", \"musicians\", \"skilled worker\", \"other\"]\ntrue_prob = [\n    [0.2, 0.2, 0.2, 0.2, 0.05, 0.05, 0.05, 0.05], # Academic orientation\n    [0.4, 0, 0.05, 0.05, 0., 0.05, 0.3, 0.15], # Technical orientation\n    [0, 0.1, 0.05, 0.05, 0.3, 0.4, 0, 0.1], # Art orientation\n]\nweights = [0.2, 0.7, 0.1]\n\nn_schools = 200\nn_students_per_school = 30\nX = []\ny_true = []\nfor i in range(n_schools):\n    school_type = generator.choice(np.arange(3), p=weights)\n    y_true.append(school_type)\n    # We simulate each graduate as going into one profession and\n    # count them in a vector\n    school_graduates = np.zeros(len(columns), dtype=\"int\")\n    for j in range(n_students_per_school):\n        profession = generator.choice(np.arange(len(columns)), p=true_prob[school_type])\n        school_graduates[profession] += 1\n    X.append(school_graduates)\n# X will be a matrix of how many students went into which direction this year\n# each school is a row, and each column is a profession\nX = np.stack(X)\ny_true = np.array(y_true)\n</code></pre>"},{"location":"count_clustering/#recovering-clusters-in-the-data","title":"Recovering clusters in the data","text":"<p>Let us assume that we don't know what clusters there are in the data (we don't know about art, academic and technical orientation), and we would like the data to tell us a) what kinds of clusters there are b) how prevalent they are c) which school is which of these orientations.</p> <p>In order to achieve this we can use a Dirichlet Multinomial Mixture model, which is a Bayesian generative model, that assumes that each of the observations (schools) is generated from one of K Dirichlet-Multinomial distributions. noloox contains a Gibbs-Sampling implementation of this model in JAX:</p> <pre><code>from noloox.mixture import DirichletMultinomialMixture\n\nmodel = DirichletMultinomialMixture(n_components=3, random_state=42)\n# We get predictions of the labels for each school\ny_pred = model.fit_predict(X)\n</code></pre>"},{"location":"count_clustering/#cluster-interpretation","title":"Cluster interpretation","text":"<p>We can check how well our model's clstering aligns with the true labels using the Adjusted Rand Score:</p> <pre><code>from sklearn.metrics import adjusted_rand_score\n\nprint(adjusted_rand_score(y_true, y_pred))\n</code></pre> <pre><code>1.0\n</code></pre> <p>We get a value of 1.0, meaning that our model fits the data perfectly and recovers the true clusters. This of course doesn't tell us which cluster is which category, so we will have to see for ourselves, which cluster predicts what probability of going into each profession. We can do this by examining the model's parameters:</p> <pre><code>import plotly.express as px\n\nprobs = (model.components_.T / model.components_.sum(axis=1)).T\npx.imshow(probs, x=columns, y=[f\"Cluster {i}\" for i in range(3)])\n</code></pre>  Probabilities for each profession in each of the discovered clusters.  <p>We can clearly see that cluster 0 corresponds to the art-school category, cluster 1 is the more academically oriented schools, while cluster 2 contains schools with a technical orientation. The model recovers these probabilities very faithfully.</p> <p>To see how likely each cluster is according to the model, we can plot the <code>weights_</code> attribute: <pre><code>fig = px.bar(x=model.weights_, y=[f\"Cluster {i}\" for i in range(3)])\nfig = fig.update_layout(xaxis_title=\"probability\", yaxis_title=\"\")\nfig.show()\n</code></pre></p>  Cluster probabilities.  <p>We can see that the model recovers the weights of the different school types roughly correctly, but slightly underestimates the probability of art schools, and overestimates the probability of technical schools. This \"rich-get-richer\" behaviour is typical of this model, and can either be mitigate by collecting more data, or adjusting its priors. See the documentation on priors here.</p>"},{"location":"n_clusters/","title":"Finding the number of clusters in the data","text":"<p>While most of the time, it is hard to make an educated guess about how many clusters there might be in a dataset ahead of time, not all clustering models can, detect the number of clusters that is in the underlying data. In this tutorial we will look at the Peax clustering model, that determines the number of clusters in a dataset based on how many peaks there are in the data's Kernel Density Estimate.</p> <p>Warning</p> <p>Peax, at the moment is only able to cluster data in two-dimensions. If you intend to use it with higher-dimensional data, we recommend that you first reduce the number of dimensions using TSNE.</p>"},{"location":"n_clusters/#data-simulation","title":"Data simulation","text":"<p>We will simulate datasets with varying numbers of clusters to see how well the model can detect the number of clusters in the dataset. We will use scikit-learn's <code>make_blobs</code> utility for this.</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_blobs\n\nrng = np.random.default_rng(42)\nn_clusters = list(range(1, 10))\nXs = []\nys = []\nfor n in n_clusters:\n    centers = rng.normal(0, 10, size=(n, 2))\n    X, y_true = make_blobs(\n        centers=centers, n_samples=1000, cluster_std=0.5, random_state=0\n    )\n    Xs.append(X)\n    ys.append(y_true)\n</code></pre> <p>Let's plot one of them to see how this roughly looks:</p> <pre><code>import plotly.express as px\n\npx.scatter(x=Xs[4][:, 0], y=Xs[4][:, 1], color=ys[4])\n</code></pre>  5 randomly generated blobs with random cluster centres.."},{"location":"n_clusters/#recovering-the-clusters","title":"Recovering the clusters","text":"<p>We will run a Peax model for each of the generated datasets, and then print the Adjusted Rand Index, to see how well the original clustering was recovered. We will also save results into a data frame so we can plot later:</p> <pre><code>from noloox.cluster import Peax\nfrom sklearn.metrics import adjusted_rand_score\nimport pandas as pd\n\nrecords = []\nfor X, y, n in zip(Xs, ys, n_clusters):\n    model = Peax(random_state=42)\n    y_pred = model.fit_predict(X)\n    n_pred = model.n_components\n    ari = adjusted_rand_score(y, y_pred)\n    print(f\"N={n}; N_pred={n_pred}; ARI={ari}\")\n    for (x0, x1), pred_label in zip(X, y_pred):\n        records.append(dict(x0=x0, x1=x1, label=str(pred_label), n=n))\nres_df = pd.DataFrame.from_records(records)\n</code></pre> <pre><code>N=1; N_pred=1; ARI=1.0\nN=2; N_pred=2; ARI=0.28885891668204566\nN=3; N_pred=3; ARI=1.0\nN=4; N_pred=4; ARI=1.0\nN=5; N_pred=5; ARI=1.0\nN=6; N_pred=5; ARI=0.8231137071695086\nN=7; N_pred=5; ARI=0.7299110696658858\nN=8; N_pred=6; ARI=0.6741328694139394\nN=9; N_pred=7; ARI=0.7897171474615344\n</code></pre> <p>We see that we recover both the number of clusters and cluster membership quite well. It seems that the most problematic one is when we have two clusters.</p> <p>Let's investigate the results visually:</p> <pre><code>px.scatter(res_df, x=\"x0\", y=\"x1\", color=\"label\", facet_col=\"n\", facet_col_wrap=3)\n</code></pre>  Clusters recovered by Peax in all datasets.  <p>We see that cluster recovery only failed when two clusters were particularly merged together or very close to each other. Now that we see the results, it also makes sense why the model failed on N=2, where the two clusters are merged together. Impressively enough the model still detected that there were two clusters here.</p>"},{"location":"robust_clustering/","title":"Outlier-Robust Clustering","text":"<p>While Gaussian Mixtures and KMeans can be powerful clustering models, their estimates can be highly affected by the presence of outliers. In this tutorial we will look at how to use a mixture of multivariate Student's T distributions to cluster observations, which is much more robust to outliers due to heavier tails in the distributions.</p>"},{"location":"robust_clustering/#data-simulation","title":"Data simulation","text":"<p>We will first generate a dataset, in which there are two clear clusters, and one of the clusters is going to contain a number of outliers, that might bias our estimate if we are not using a robust model. We will use scikit-learn's <code>make_blobs</code> convenience function for this:</p> <pre><code>from sklearn.datasets import make_blobs\nimport numpy as np\n\nX, y_true = make_blobs(centers=[[2, 2], [-2, -2]], n_samples=1000, cluster_std=0.5)\n# Adding 50 outliers:\nX_outliers, y_outliers = make_blobs(centers=[[5, 5]], n_samples=50, cluster_std=0.1)\nX = np.concatenate([X, X_outliers])\ny_true = np.concatenate([y_true, y_outliers])\n</code></pre> <p>Let us plot the data to get a feel for how it looks:</p> <pre><code>import plotly.express as px\n\nfig = px.scatter(x=X[:, 0], y=X[:, 1], color=list(map(str, y_true)))\nfig.show()\n</code></pre>  Two clear clusters with 50 outliers in one of them.  <p>We see that cluster 0 has some outliers, that might bias our estimate.</p>"},{"location":"robust_clustering/#model-fitting","title":"Model fitting","text":"<p>Let's fit a Gaussian Mixture model and a Student's t Mixture model to our data and save the predictions:</p> <pre><code>from sklearn.mixture import GaussianMixture\nfrom noloox.mixture import StudentsTMixture\n\ngmm = GaussianMixture(2)\ny_gaussian = gmm.fit_predict(X)\n\nstmm = StudentsTMixture(2, df=1.0)\ny_t = stmm.fit_predict(X)\n</code></pre>"},{"location":"robust_clustering/#model-criticism","title":"Model criticism","text":"<p>To see how well the models' predictions align with or true labels, we can look at the Adjusted Rand Index between the true and predicted clusterings.</p> <pre><code>from sklearn.metrics import adjusted_rand_score\n\nprint(\"ARI(Gaussian): \", adjusted_rand_score(y_true, y_gaussian))\nprint(\"ARI(Student's T): \", adjusted_rand_score(y_true, y_t))\n</code></pre> <pre><code>ARI(Gaussian):  1.0\nARI(Student's T):  1.0\n</code></pre> <p>We see that both models' labels perfectly match the true labels, meaning, they identified the correct clusters.</p> <p>Now let's examine how close each models' estimate lies to the true cluster centres:</p> <pre><code>import plotly.graph_objects as go\n\nfig = go.Figure()\nfig = fig.add_scatter(\n    x=X[:, 0],\n    y=X[:, 1],\n    mode=\"markers\",\n    marker=dict(size=5, color=\"black\"),\n    opacity=0.5,\n    showlegend=False,\n)\nfig = fig.add_scatter(\n    x=gmm.means_[:, 0],\n    y=gmm.means_[:, 1],\n    name=\"GMM cluster centres\",\n    mode=\"markers\",\n    marker=dict(\n        color=\"white\", size=15, symbol=\"cross\", line=dict(width=2, color=\"black\")\n    ),\n    zorder=10,\n)\nfig = fig.add_scatter(\n    x=stmm.means_[:, 0],\n    y=stmm.means_[:, 1],\n    name=\"StudentsTMixture cluster centres\",\n    mode=\"markers\",\n    marker=dict(color=\"white\", size=15, symbol=\"x\", line=dict(width=2, color=\"black\")),\n    zorder=10,\n)\nfig = fig.add_scatter(\n    x=[2, -2],\n    y=[2, -2],\n    name=\"True cluster centers\",\n    mode=\"markers\",\n    marker=dict(\n        color=\"white\", size=15, symbol=\"star\", line=dict(width=2, color=\"black\")\n    ),\n    zorder=10,\n)\nfig.show()\n</code></pre>  Cluster centroids identified by GMM and StudentsTMixture compared to true cluster means.  <p>We can see that StudentsTMixture identified the cluster centres much more faithfully. The Gaussian Mixture model's estimate of the cluster means has been substantially affected by the presence of outliers</p>"},{"location":"senstopic/","title":"Topic discovery by factoring transformer embeddings","text":"<p>In older topic models, one would typically cluster or factorize bag-of-words matrices using LDA or NMF. </p> <p>Info</p> <p>See Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation in the scikit-learn docs or Topic modelling for short texts for more information on BoW topic modelling.</p> <p>In contrast, modern topic models, like BERTopic cluster embeddings from sentence-transformers. While clustering them is easy, factorizing these embeddings in a nonnegative topic-space is not trivial, as the embeddings themselves are unbounded, and can take on positive or negative values.</p> <p>In this tutorial we will look at how you can achieve this using Semi-Nonnegative Matrix Factorization.</p> <p>Tip</p> <p>This model is actually called SensTopic, and is implemented in the Turftopic Python library with much more complete functionality. If you intend to use this model in practice, you should probably use that implementation, they are based on the same SNMF model. This tutorial is strictly here to demonstrate how you can produce positive factors over unbounded data using SNMF.</p>"},{"location":"senstopic/#data-loading","title":"Data loading","text":"<p>We will use a subset of the 20 Newsgroups dataset from scikit-learn for this tutorial:</p> <pre><code>from sklearn.datasets import fetch_20newsgroups\n\ncorpus = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    categories=[\n      \"comp.graphics\",\n      \"comp.os.ms-windows.misc\",\n      \"comp.sys.ibm.pc.hardware\",\n      \"comp.sys.mac.hardware\",\n      \"comp.windows.x\"\n    ]\n).data\n</code></pre>"},{"location":"senstopic/#term-extraction","title":"Term extraction","text":"<p>In order for us to be able to estimate keyword importance for topics, we will need to extract all terms in the corpus. We will do this by getting the vocabulary of a fitted CountVectorizer.</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(min_df=10)\nvectorizer.fit(corpus)\nvocab = vectorizer.get_feature_names_out()\n</code></pre>"},{"location":"senstopic/#producing-transformer-embeddings","title":"Producing transformer embeddings","text":"<pre><code>from sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = encoder.encode(corpus, show_progress_bar=True)\nvocab_embeddings = encoder.encode(vocab, show_progress_bar=True)\n</code></pre> <pre><code>from noloox.decomposition import SNMF\n\nmodel = SNMF(n_components=10, sparsity=1.0)\ndoc_topic_matrix = model.fit_transform(embeddings)\n\ntopic_word_matrix = model.transform(vocab_embeddings).T\n</code></pre> <pre><code>for i, comp in enumerate(topic_word_matrix):\n    top = np.argsort(-comp)[:10]\n    print(f\"Topic {i}:\", \", \".join(vocab[top]))\n</code></pre> Topic ID Top Words Topic 0 modem, modems, connecting, telnet, ports, port, connect, ethernet, connects, connection Topic 1 processors, processor, cpus, cpu, performance, benchmarks, pentium, cheaper, intel, efficient Topic 2 vga, monitors, monitor, 640x480, displays, resolution, resolutions, lcd, 1280x1024, screen Topic 3 uh, um, so, em, yeah, er, ah, but, oh, and Topic 4 windows, win3, os, microsoft, openwin, win31, openwindows, netware, ms, executables Topic 5 printing, printers, printer, prints, print, laserwriter, printed, laserjet, ink, deskjet Topic 6 hdd, disk, harddisk, disks, drives, fdisk, seagate, hd, drive, partition Topic 7 xcreatewindow, xtwindow, xtrealizewidget, xterminal, xterm, xtpointer, xserver, xservers, xt, xdm Topic 8 bitmaps, colormaps, bitmap, imagemagick, colormap, animation, animations, imagewriter, xputimage, photoshop Topic 9 archived, mailed, published, contact, subscription, contacted, archive, publications, incorporated, email"},{"location":"short_topic_modelling/","title":"Topic modelling for short texts","text":"<p>Latent Dirichlet Allocation is a very popular model for finding topics in text documents. Crucially, however, LDA is very bad at discovering topics in short documents. This is a problem when you are trying to model topics in Tweets or forum posts, because this sort of content is typically short-form.</p> <p>The reason LDA is so bad at this, is because it assumes that every document contains multiple topics, while this is usually not the case with shorter documents. We can, instead, assume that each document comes from one underlying cluster that determines word probabilities. This model is called a Dirichlet-Multinomial Mixture, and can be used for clustering text, as well as uncovering topics.</p> <p>In this tutorial we will look at how you can use DirichletMultinomialMixture to find topics in short texts.</p>"},{"location":"short_topic_modelling/#data-loading","title":"Data Loading","text":"<p>We are going to use a subset of the 20 Newsgroups dataset from scikit-learn. We only load the alt.atheism forum data for now so that it doesn't take a long time to run the algorithm.</p> <pre><code>from sklearn.datasets import fetch_20newsgroups\n\ncorpus = fetch_20newsgroups(\n    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=[\"alt.atheism\"]\n).data\n</code></pre>"},{"location":"short_topic_modelling/#preprocessing","title":"Preprocessing","text":"<p>We will use scikit-learn's CountVectorizer to extract a Bag-of-words matrix over our texts. We filter for too frequent or infrequent words, and filter out stop-words as well.</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(\n    min_df=10, max_df=0.1, max_features=4000, stop_words=\"english\"\n)\nX = vectorizer.fit_transform(corpus)\n</code></pre>"},{"location":"short_topic_modelling/#model-fitting","title":"Model fitting","text":"<p>We can now fit a Dirichlet Multinomial Mixture to our data. I chose to use 5 topics, since the results will be easy to display.</p> <pre><code>from noloox.mixture import DirichletMultinomialMixture\n\nmodel = DirichletMultinomialMixture(5).fit(X)\n</code></pre>"},{"location":"short_topic_modelling/#model-interpretation","title":"Model Interpretation","text":"<p>We plot the top 10 words for each topic on bar charts using Plotly to understand what topics mean.</p> <pre><code>from plotly.subplots import make_subplots\nimport numpy as np\n\nfig = make_subplots(rows=1, cols=5, subplot_titles=[f\"Topic {i}\" for i in range(5)])\nvocab = vectorizer.get_feature_names_out()\nfor i, comp in enumerate(model.components_):\n    top = np.argsort(-comp)[:10]\n    fig.add_bar(\n        y=vocab[top][::-1],\n        x=comp[top][::-1],\n        row=1,\n        col=i+1,\n        orientation=\"h\",\n        showlegend=False,\n    )\nfig.show()\n</code></pre>  Top words in each topic learned by our model."}]}